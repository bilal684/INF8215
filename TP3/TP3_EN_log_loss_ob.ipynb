{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 8215 - Intelligence artif.: méthodes et algorithmes \n",
    "## Fall 2018 - TP3 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due date: December 6**\n",
    "\n",
    "**Files to submit:**\n",
    "    * TP3_EN.ipynb filled\n",
    "    * SoftmaxClassifier.py filled\n",
    "    * test_prediction.csv prediction file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to give you an overview of the general course of a machine learning project while familiarizing you with adapted python libraries.\n",
    "\n",
    "\n",
    "In the first part, you will implement a multi-class classification algorithm called **softmax regression** using only **numpy** library and embed it in the **scikit-learn** library.\n",
    "\n",
    "In the second part, you will learn about the **dataset** used for this project. Moreover, you will have to perform the **preprocessing** of these data so that it can be used in conventional machine learning algorithms. To this end, you will use **pandas** and **scikit-learn** libraries.\n",
    "\n",
    "Finally, in the third part, you will compare the efficiency of the model that you have implemented with other models already implemented in **scikit-learn**. Then you will try to improve the performance of the selected algorithm.\n",
    "\n",
    "Once all these steps are done, you will submit your results on the **kaggle** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install **pandas** and **scikit-learn**, the easiest way is to download and install **Anaconda**, which groups together the most used packages for scientific computing and data science.\n",
    "\n",
    "You will find the distribution here: https://www.anaconda.com/download/#linux.\n",
    "\n",
    "Make sure you have **scikit-learn** **20.0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Competition (2 points)\n",
    "\n",
    "When you finish the lab, you can submit your predictions on **kaggle**, you will get your performance in terms of **log loss**.\n",
    "You can then send me this result by email (laurent.boucaud@polymtl.ca) and join your prediction file on the test set (for verification).\n",
    "\n",
    "A conversation in the forum will be created to keep up to date the best score obtained by one of the teams of the course.\n",
    "\n",
    "As long as no forum is created, **do not send me your performances if they are above 0.8 of log loss**.\n",
    "\n",
    "Once the first best score posted in the forum, **only give me your results if your log loss is lower than the previous best score**.\n",
    "\n",
    "The number of points obtained will be proportional to the ranking of the teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Regression (10 points)\n",
    "\n",
    "\n",
    "In this part you will implement **softmax regression**, the **logistic regression** variant which allows you to perform classification for a class number greater than 2.\n",
    "\n",
    "The code to be completed is in the **SoftmaxClassifier.py** file.\n",
    "\n",
    "**For this exercise, the constraint is to use only the numpy library **\n",
    "\n",
    "## Sklearn encapsulation\n",
    "\n",
    "\n",
    "The class **SoftmaxClassifier** inherits from the **BaseEstimator** and **ClassifierMixin** classes from **scikit-learn** which will allow us to easily use the tools provided by scikit-learn with our classifier later.\n",
    "\n",
    "For compatibility, the classifier necessarily implements the methods:\n",
    "\n",
    "* **fit**: responsible for training the model\n",
    "* **predict_proba**: Predicts the probability of each class for each example in the dataset provided.\n",
    "* **predict**: Predicts the class for each example in the provided dataset.\n",
    "* **score**: quantifies the difference between the predicted classes and the actual classes for the dataset provided\n",
    "\n",
    "\n",
    "## Train/Test set:\n",
    "\n",
    "When one wants to test the performance of learning a machine learning algorithm, one **does not test it on the data used for learning**.\n",
    "\n",
    "Indeed, what interests us is that our algorithm is able to generalize its predictions to the data that it has never seen.\n",
    "\n",
    "To illustrate, if we test an algorithm on the training data, we test its ability to **learn by heart** the dataset and not to **generalize**.\n",
    "\n",
    "Therefore, when receiving a new dataset, the first thing to do is to **split it into two parts**: a **train set** (**70-80%** of the dataset) and a **test set** (**20-30%** of the dataset).\n",
    "\n",
    "All **data processing** and **learning algorithms** should be learned only on the training set and then applied to the test set.\n",
    "\n",
    "By doing so, the lack of prior knowledge of the test set during training is ensured.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is an algorithm that allows finding the optimal solution of a certain number of problems. The principle is as follows: we define a **cost function J** that characterizes the problem.\n",
    "This function depends on a set of **$\\theta$** parameters. Gradient descent seeks to **minimize** the cost function by **iteratively modifying** the parameters.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "The cost function gradient for a given $\\theta$, is the direction in which $\\theta$ must be modified to reduce the value of the cost function.\n",
    "\n",
    "The cost function is minimal when the gradient is zero.\n",
    "\n",
    "Concretely, we initialize $\\theta$ randomly, and we do at each iteration a step to reduce the cost function until convergence of the algorithm to a minimum of the cost function.\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "\n",
    "The learning rate represents the size of the step that will be made in the direction of the gradient.\n",
    "The larger it is, the faster the convergence, but there is a risk that the algorithm will diverge.\n",
    "\n",
    "The smaller it is, the slower the convergence.\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "There are several gradient descent algorithms. We will use Batch gradient descent.\n",
    "\n",
    "In this algorithm, before updating $\\theta$, we calculate the gradients on all the training examples.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "This is a step of the gradient descent, a single gradient update.\n",
    "\n",
    "### Bias/Variance tradeoff\n",
    "\n",
    "When training a machine learning algorithm we look for a tradeoff between **bias** and **variance**.\n",
    "\n",
    "A model with a **strong bias**, is a model that is **too simple** for the given data structure (e.g., a linear model for quadratic data), this limits the capacity of the model to generalize. We also call bias  **underfitting**.\n",
    "\n",
    "A model with a **high variance** means that it is sensitive to small variations in training data, this corresponds to **overfitting**, i.e., the model is too close to the structure of the training set which **limits its ability to generalize**.\n",
    "\n",
    "A model with a **significant bias** will have a **poor** performance over the **training set**.\n",
    "A model with a **significant variance** will have a much worse **performance** on the entire **test set** than on the  **train set**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "\n",
    "In machine learning to represent a vector of categorical data, we use one-hot encoding.\n",
    "\n",
    "For a vector containing 5 examples and 3 different categories, it is represented as a matrix of size 5 by 3. This matrix is entirely filled with 0 except for the index corresponding to the number of the class for each example.\n",
    "\n",
    "\n",
    "For example\n",
    "$ y = \\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "becomes:\n",
    "\n",
    "$ yohe =  \\left(\\begin{array}{cc} \n",
    "1. & 0. & 0.\\\\\n",
    "1. & 0. & 0.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "0. & 0. & 1.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "\n",
    "#### Question 1 (1 point)\n",
    "Implement the **_one_hot** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight matrix\n",
    "\n",
    "Let $ X_{m * n} $ be the example matrix and $ \\Theta _{n*K} $ the weight matrix with:\n",
    "\n",
    "* **m** number of examples\n",
    "* **n** number of features\n",
    "* **k** number of target classes\n",
    "\n",
    "\n",
    "\n",
    "It is common to add an additional column to X, this column is filled with 1. To take into account this change, we must add a line to the matrix $\\Theta$.\n",
    "\n",
    "We get X_bias$_{m*(n+1)}$ et $ \\Theta _{(n+1)*K} $\n",
    "\n",
    "\n",
    "Intuitively, each class K is associated with a $\\theta$ column.\n",
    "\n",
    "We denote by $\\theta_k$ (n+1 dimension vector) the weight column associated with the prediction of class k .\n",
    "\n",
    "$\\Theta$ = [$\\theta_0$,$\\theta_1$... $\\theta_k$ ... $\\theta_n$ ]\n",
    "\n",
    "Thus $ z = x * \\Theta $ gives a vector of dimension K which are **logits** associated with x for each class.\n",
    "\n",
    "#### Question 2 (1 point)\n",
    "\n",
    "In the **fit** function in SoftmaxClassifier.py instantiate X_bias and initialize $\\Theta$  randomly. (line 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We want to convert the logit vector **z** obtained in the previous part into a **probability vector**.\n",
    "\n",
    "For this we define the **softmax function**:\n",
    "\n",
    "\n",
    "$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n",
    "\n",
    "\n",
    "Intuitively, for a logit of z, $z_k$, we take the exponential of this value and divide it by the sum of the exponentials of each logit of the vector **z**. We get $\\hat{p_x}^k$ the probability that the example **x** belongs to the class **k**.\n",
    "\n",
    "The operation is repeated for each logit of the vector **z**.\n",
    "\n",
    "We thus obtain a probability vector $\\hat{p_x}$ for an example **x**.\n",
    "\n",
    "The division makes it possible to make the sum of the terms of the vector $\\hat{p_x}$ equal to 1 which is indispensable for probabilities.\n",
    "\n",
    "#### Question 3 (1 point)\n",
    "Implement  **_softmax** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (1 point)\n",
    "\n",
    "Using the **_ softmax** function of question 3, implement the **predict_proba** and **predict** methods in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût Log loss\n",
    "\n",
    "Let log loss (ou cross entropy) be the cost function:\n",
    "\n",
    "$$ J( \\Theta) = \\frac{-1}{m}\\sum_{\\substack{1<i<m}} \\sum_{\\substack{1<k<K}} y_k^i log( \\hat{p_k}^i ) $$\n",
    "\n",
    "with:\n",
    "* **K** number of classes\n",
    "* **m** number of examples\n",
    "* $ \\hat{p_k}^i  $  probability that example i be of target class k\n",
    "* $y_k^i$ is 1 if the target class of example i is k, 0 otherwise\n",
    "\n",
    "**Implementation detail:** Cost function is not defined for probabilities taking values 0. or 1., we must ensure that given $\\epsilon$, probabilities are in  [$\\epsilon$, 1. - $\\epsilon$].\n",
    "#### Question 5 (1 point)\n",
    "\n",
    "Implement the **_ cost_function** method in SoftmaxClassifier.py by taking into account the **implementation detail** (self.eps variable) and use it to calculate the **loss** variable in the **fit** method (line 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function gradient\n",
    "\n",
    "The **gradient of J** with respect to $\\theta_k$ is :\n",
    "\n",
    "\n",
    "$$ \\Delta_{\\theta_k}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p_k}^i - y_k^i)x^i  $$\n",
    "\n",
    "with:\n",
    "* **K** number of target classes\n",
    "* **m** number of examples\n",
    "* $ \\hat{p_k}^i  $  probability that example i is of class k\n",
    "* $y_k^i$ is 1 if example i target class is k, 0 otherwise\n",
    "\n",
    "\n",
    "We can rewrite it as matrices, the **gradient of J** with respect to $\\Theta$** is :\n",
    "$$ \\Delta_J( \\Theta) = \\frac{1}{m} X_{bias}^T *( \\hat{p} - y_{ohe}) $$\n",
    "\n",
    "with:\n",
    "\n",
    "* $\\hat{p}$ predicted probability matrix for every example and every class\n",
    "* $y_{ohe}$ one-hot encoded y\n",
    "* $X_{bias}^T$  Transposed matrix of $X_{bias}$\n",
    "* **\\*** Dot product\n",
    "\n",
    "#### Question 6 (1 point)\n",
    "Implement  **_get_gradient** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights update\n",
    "\n",
    "When the gradient has been computed, we must update the weights with these gradients.\n",
    "\n",
    "\n",
    "$$ \\Theta  = \\Theta - \\gamma \\Delta J( \\Theta) $$\n",
    "\n",
    "\n",
    "with:\n",
    "* $\\Theta$ weight matrix\n",
    "* $\\gamma$  learning rate\n",
    "* $\\Delta J( \\Theta)$ gradient of $J( \\Theta)$ with respect to $\\Theta$\n",
    "\n",
    "#### Question 7 (1 point)\n",
    "Update **self.theta_** in the **fit** method in SoftmaxClassifier.py (line 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "To limit **overfitting**, we use the regularization, we add a term to the function of cost $J( \\Theta)$.\n",
    "\n",
    "This term will add constraints on the weight of the model during training.\n",
    "We will use the **L2** regularization:\n",
    "\n",
    "\n",
    "$$ L2(\\Theta) = \\alpha \\sum_{\\substack{1<=i<n}} \\sum_{\\substack{0<=k<K}} \\theta_{i,k}^2 $$ \n",
    "\n",
    "with:\n",
    "\n",
    "* $\\alpha$ regularization coefficient\n",
    "\n",
    "**Note:** The first sum does not start at 0 but at 1 because we do not adjust the weights associated with the X bias column.\n",
    "\n",
    "Adding this term leads the model to learn the data while keeping its weight as small as possible.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 8 (1 point)\n",
    "\n",
    "Modify the methods **_ get_gradient** and **_ cost_function** to take into account the regularization when the boolean self.regularization is true in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (1 point)\n",
    "\n",
    "The regularization term is used only during training. When one wants to evaluate the performance of the model **after training**, one uses the **non-regulated** cost function.\n",
    "\n",
    "Implement the **score** function that evaluates the quality of the prediction **after training** in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Too many **epochs** can result in **overfitting**.\n",
    "To overcome this problem, we can use the mechanism of **early stopping**.\n",
    "This aims to stop the training if the difference in the cost function between two **consecutive epochs** is less than a defined **threshold**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 10 (1 point)\n",
    "\n",
    "Finish implementing the **fit** function by adding the **early stopping** mechanism when the **self.early_stopping** boolean is true. The threshold is given by the **self.threshold variable** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00709478, -0.3125352 , -3.08128635, -1.60748412, -1.11516814],\n",
       "       [-0.01271932, -0.26197073, -2.38289833, -1.57234519, -0.79567503],\n",
       "       [-0.10226082, -0.14878639, -1.07348192, -0.83121456, -0.58537832],\n",
       "       [-0.23218625, -1.47304081, -0.20543059, -2.72412026, -4.42139681]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xxx=np.random.rand(4,5)\n",
    "np.log(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the solution:\n",
    "\n",
    "The code below imports the **iris** multiclass  classification dataset available on sklearn. The data is divided into two parts, the training set and the test set, and then they are normalized.\n",
    "\n",
    "The classifier implemented in the **SoftmaxClassifier.py** file is imported and then trained on the training set and tested on the test set.\n",
    "\n",
    "The purpose of this part is just to check your implementation **when you are sure your code is working**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SoftmaxClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"A softmax classifier\"\"\"\n",
    "\n",
    "    def __init__(self, lr = 0.1, alpha = 1, n_epochs = 1000, eps = 1.0e-5,threshold = 1.0e-5 , regularization = True, early_stopping = True):\n",
    "       \n",
    "        \"\"\"\n",
    "            self.lr : the learning rate for weights update during gradient descent\n",
    "            self.alpha: the regularization coefficient \n",
    "            self.n_epochs: the number of iterations\n",
    "            self.eps: the threshold to keep probabilities in range [self.eps;1.-self.eps]\n",
    "            self.regularization: Enables the regularization, help to prevent overfitting\n",
    "            self.threshold: Used for early stopping, if the difference between losses during \n",
    "                            two consecutive epochs is lower than self.threshold, then we stop the algorithm\n",
    "            self.early_stopping: enables early stopping to prevent overfitting\n",
    "        \"\"\"\n",
    "\n",
    "        self.lr = lr \n",
    "        self.alpha = alpha\n",
    "        self.n_epochs = n_epochs\n",
    "        self.eps = eps\n",
    "        self.regularization = regularization\n",
    "        self.threshold = threshold\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "    \"\"\"\n",
    "        Public methods, can be called by the user\n",
    "        To create a custom estimator in sklearn, we need to define the following methods:\n",
    "        * fit\n",
    "        * predict\n",
    "        * predict_proba\n",
    "        * fit_predict        \n",
    "        * score\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        In:\n",
    "        X : the set of examples of shape nb_example * self.nb_features\n",
    "        y: the target classes of shape nb_example *  1\n",
    "\n",
    "        Do:\n",
    "        Initialize model parameters: self.theta_\n",
    "        Create X_bias i.e. add a column of 1. to X , for the bias term\n",
    "        For each epoch\n",
    "            compute the probabilities\n",
    "            compute the loss\n",
    "            compute the gradient\n",
    "            update the weights\n",
    "            store the loss\n",
    "        Test for early stopping\n",
    "\n",
    "        Out:\n",
    "        self, in sklearn the fit method returns the object itself\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        prev_loss = np.inf\n",
    "        self.losses_ = []\n",
    "\n",
    "        self.nb_feature = X.shape[1]\n",
    "        self.nb_classes = len(np.unique(y))\n",
    "        tmp=np.ones((X.shape[0],1))\n",
    "        X_bias = np.concatenate((tmp,X),axis=1)\n",
    "        self.theta_=np.random.rand(self.nb_feature+1,self.nb_classes)\n",
    "        for epoch in range( self.n_epochs):\n",
    "            probabilities =self.predict_proba(X) \n",
    "            loss =  self._cost_function(probabilities,y)\n",
    "            gradient=self._get_gradient(X,y,probabilities)\n",
    "            self.theta_ = self.theta_-self.lr*gradient \n",
    "            self.losses_.append(loss)\n",
    "            if self.early_stopping:\n",
    "                if epoch>1 and abs(self.losses_[epoch]-self.losses_[epoch-1])<self.threshold:\n",
    "                    break\n",
    "        return self\n",
    "    \"\"\"\n",
    "        In: \n",
    "        X without bias\n",
    "\n",
    "        Do:\n",
    "        Add bias term to X\n",
    "        Compute the logits for X\n",
    "        Compute the probabilities using softmax\n",
    "\n",
    "        Out:\n",
    "        Predicted probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"theta_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        tmp=np.ones((X.shape[0],1))\n",
    "        X_bias = np.concatenate((tmp,X),axis=1)\n",
    "        z=np.dot(X_bias,self.theta_)\n",
    "        return self._softmax(z)\n",
    "        \"\"\"\n",
    "        In: \n",
    "        X without bias\n",
    "\n",
    "        Do:\n",
    "        Add bias term to X\n",
    "        Compute the logits for X\n",
    "        Compute the probabilities using softmax\n",
    "        Predict the classes\n",
    "\n",
    "        Out:\n",
    "        Predicted classes\n",
    "    \"\"\"\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"theta_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        predicted_prob=self.predict_proba(X,None)\n",
    "        result=np.argmax(predicted_prob,axis=1)\n",
    "        return result.reshape(result.shape[0],1)\n",
    "    def fit_predict(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.predict(X,y)\n",
    "    \"\"\"\n",
    "        In : \n",
    "        X set of examples (without bias term)\n",
    "        y the true labels\n",
    "\n",
    "        Do:\n",
    "            predict probabilities for X\n",
    "            Compute the log loss without the regularization term\n",
    "\n",
    "        Out:\n",
    "        log loss between prediction and true labels\n",
    "\n",
    "    \"\"\"    \n",
    "    def score(self, X, y=None):\n",
    "        sum_number=X.shape[0]\n",
    "        result=self.predict(X,y)\n",
    "        tmp=result-y\n",
    "        right_number=sum(tmp==0)\n",
    "        return right_number[0]/sum_number\n",
    "    \"\"\"\n",
    "        Private methods, their names begin with an underscore\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "        In :\n",
    "        y without one hot encoding\n",
    "        probabilities computed with softmax\n",
    "\n",
    "        Do:\n",
    "        One-hot encode y\n",
    "        Ensure that probabilities are not equal to either 0. or 1. using self.eps\n",
    "        Compute log_loss\n",
    "        If self.regularization, compute l2 regularization term\n",
    "        Ensure that probabilities are not equal to either 0. or 1. using self.eps\n",
    "\n",
    "        Out:\n",
    "        Probabilities\n",
    "    \"\"\"\n",
    "    def _cost_function(self,probabilities, y ):\n",
    "        m=probabilities.shape[0]\n",
    "        p_processed=np.clip(probabilities,self.eps,1-self.eps)\n",
    "        cost_sum=0\n",
    "        y_one_hot=self._one_hot(y)\n",
    "        for i in range(m):\n",
    "            c = np.argmax(y_one_hot[i,:]) # class\n",
    "            cost_sum += np.log(p_processed[i,c])\n",
    "        if self.regularization:\n",
    "            cost_sum=(-1/m)*cost_sum\n",
    "            cost_sum+=(1/m)*self.alpha*np.sum(self.theta_[1:,:]**2)\n",
    "            return cost_sum\n",
    "        else:\n",
    "            return (-1/m)*cost_sum\n",
    "    \"\"\"\n",
    "        In :\n",
    "        Target y: nb_examples * 1\n",
    "\n",
    "        Do:\n",
    "        One hot-encode y\n",
    "        [1,1,2,3,1] --> [[1,0,0],\n",
    "                         [1,0,0],\n",
    "                         [0,1,0],\n",
    "                         [0,0,1],\n",
    "                         [1,0,0]]\n",
    "        Out:\n",
    "        y one-hot encoded\n",
    "    \"\"\"\n",
    "    def _one_hot(self,y):\n",
    "        tmp=np.zeros((y.shape[0],self.nb_classes))\n",
    "        for i in range(y.shape[0]):\n",
    "            invoked_classc=int(y[i][0])\n",
    "            tmp[i][invoked_classc]=1\n",
    "        return tmp\n",
    "    \"\"\"\n",
    "        In :\n",
    "        Logits: (self.nb_features +1) * self.nb_classes\n",
    "\n",
    "        Do:\n",
    "        Compute softmax on logits\n",
    "\n",
    "        Out:\n",
    "        Probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def _softmax(self,z):\n",
    "        prob_vector=np.exp(z)\n",
    "        tmp_sum=np.sum(prob_vector,axis=1)\n",
    "        tmp_sum=tmp_sum.reshape(tmp_sum.shape[0],1)\n",
    "        return prob_vector/tmp_sum\n",
    "    \"\"\"\n",
    "        In:\n",
    "        X with bias\n",
    "        y without one hot encoding\n",
    "        probabilities resulting of the softmax step\n",
    "\n",
    "        Do:\n",
    "        One-hot encode y\n",
    "        Compute gradients\n",
    "        If self.regularization add l2 regularization term\n",
    "\n",
    "        Out:\n",
    "        Gradient\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_gradient(self,X,y, probas):\n",
    "        if self.regularization:\n",
    "            m=y.shape[0]\n",
    "            tmp=np.ones((X.shape[0],1))\n",
    "            X_bias = np.concatenate((tmp,X),axis=1)\n",
    "            tmp_labels=self._one_hot(y)\n",
    "            tmp_subtraction=probas-tmp_labels\n",
    "            gradients_costfunction=(1/m)*(np.dot((np.transpose(X_bias)),tmp_subtraction))\n",
    "            gradients_costfunction[1:,:]+=2*self.alpha*self.theta_[1:,:]/m\n",
    "        else:\n",
    "            m=y.shape[0]\n",
    "            tmp=np.ones((X.shape[0],1))\n",
    "            X_bias = np.concatenate((tmp,X),axis=1)\n",
    "            gradients_costfunction=(1/m)*np.dot((np.transpose(X_bias)),probas-self._one_hot(y))\n",
    "        return gradients_costfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "data,target =load_iris().data,load_iris().target\n",
    "\n",
    "# split data in train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# standardize columns using normal distribution\n",
    "# fit on X_train and not on X_test to avoid Data Leakage\n",
    "s = StandardScaler()\n",
    "X_train = s.fit_transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "# import the custom classifier\n",
    "cl = SoftmaxClassifier()\n",
    "y_train=y_train.reshape((y_train.shape[0],1))\n",
    "# train on X_train and not on X_test to avoid overfitting\n",
    "train_p = cl.fit_predict(X_train,y_train)\n",
    "test_p = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get relatively close values for the test and training set, and they are at least greater than 0.8, your model should be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (0.94201680672268895, 0.94201680672268895, 0.94201680672268895, None)\n",
      "test : (0.97916666666666663, 0.97916666666666663, 0.978494623655914, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# display precision, recall and f1-score on train/test set\n",
    "print(\"train : \"+ str(precision_recall_fscore_support(y_train, train_p,average = \"macro\")))\n",
    "print(\"test : \"+ str(precision_recall_fscore_support(y_test, test_p,average = \"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5268269963513343, 1.3809523168821527, 1.2563027346226148, 1.1508276696399946, 1.0621454389289937, 0.98778886799238941, 0.9254103881395459, 0.87291275955393044, 0.8285047405212923, 0.79070190212784763, 0.7582968451484754, 0.73031749528791678, 0.70598458882132931, 0.68467360047691928, 0.6658828383395925, 0.64920766295107679, 0.63432006725000933, 0.62095266813447014, 0.60888621085224204, 0.59793982135042456, 0.58796338902514966, 0.57883159502739601, 0.5704392111493477, 0.56269738145718806, 0.55553066637518156, 0.54887468062795375, 0.54267419578586329, 0.53688160802264373, 0.53145569437222351, 0.52636059802425905, 0.52156499636156561, 0.51704141552265592, 0.51276566302275162, 0.50871635595122089, 0.50487452690550383, 0.50122329343877969, 0.49774757963023386, 0.49443388061349586, 0.49127006265783268, 0.48824519279240125, 0.4853493930761667, 0.4825737155063185, 0.47991003427348899, 0.47735095264941552, 0.47488972226045334, 0.47252017288073495, 0.47023665118933283, 0.46803396719020535, 0.46590734720291943, 0.46385239250474514, 0.46186504284760788, 0.45994154419207689, 0.45807842009942584, 0.4562724463054752, 0.45452062806917287, 0.45282017994714091, 0.45116850769450983, 0.44956319203388911, 0.44800197406952902, 0.44648274215364792, 0.44500352003740218, 0.44356245616076706, 0.4421578139542604, 0.44078796304145873, 0.43945137124505118, 0.43814659731106426, 0.43687228427617891, 0.4356271534119639, 0.43440999868759045, 0.43321968169931568, 0.43205512702090187, 0.43091531793424931, 0.42979929250402082, 0.42870613996396933, 0.42763499738612903, 0.42658504660708579, 0.42555551138822412, 0.42454565478921391, 0.42355477673610886, 0.42258221176728367, 0.4216273269420856, 0.42068951989856107, 0.41976821704790485, 0.41886287189446902, 0.41797296347119778, 0.41709799488129812, 0.41623749193778498, 0.41539100189329847, 0.41455809225326468, 0.41373834966607315, 0.41293137888449982, 0.41213680179309614, 0.41135425649671009, 0.41058339646570774, 0.40982388973383521, 0.40907541814499049, 0.40833767664547055, 0.40761037261854327, 0.40689322525843408, 0.40618596498104831, 0.4054883328689613, 0.40480008014838481, 0.40412096769600758, 0.40345076557375259, 0.40278925258964737, 0.40213621588312748, 0.4014914505332261, 0.40085475918820423, 0.4002259517152858, 0.39960484486924996, 0.39899126197872947, 0.39838503264913083, 0.39778599248118318, 0.39719398280417051, 0.39660885042298488, 0.39603044737818183, 0.39545863071828169, 0.39489326228360605, 0.39433420850098577, 0.39378134018872329, 0.39323453237122485, 0.39269366410276146, 0.3921586182998491, 0.39162928158177057, 0.39110554411879001, 0.39058729948764437, 0.39007444453390971, 0.3895668792408809, 0.38906450660460939, 0.38856723251477576, 0.38807496564108745, 0.38758761732491293, 0.38710510147587873, 0.38662733447317355, 0.38615423507131674, 0.38568572431016179, 0.38522172542892363, 0.38476216378402178, 0.3843069667705511, 0.38385606374719955, 0.38340938596443863, 0.38296686649583001, 0.38252844017229232, 0.38209404351918469, 0.38166361469607202, 0.38123709343904055, 0.38081442100544444, 0.38039554012096466, 0.37998039492887287, 0.37956893094139604, 0.37916109499308376, 0.37875683519608205, 0.3783561008972306, 0.37795884263689578, 0.37756501210945953, 0.37717456212539108, 0.37678744657482749, 0.37640362039259739, 0.37602303952462035, 0.37564566089562085, 0.37527144237810267, 0.37490034276252132, 0.37453232172860618, 0.37416733981778127, 0.37380535840663703, 0.37344633968140628, 0.37309024661340084, 0.37273704293537124, 0.37238669311874462, 0.37203916235170825, 0.37169441651809965, 0.37135242217707121, 0.37101314654349671, 0.3706765574690869, 0.37034262342418789, 0.37001131348023092, 0.3696825972928095, 0.36935644508535537, 0.36903282763339257, 0.36871171624934151, 0.36839308276785548, 0.36807689953166489, 0.36776313937791061, 0.36745177562494646, 0.3671427820595915, 0.36683613292481609, 0.36653180290784337, 0.36622976712864852, 0.36593000112884622, 0.3656324808609413, 0.36533718267793952, 0.36504408332329563, 0.3647531599211925, 0.36446438996713282, 0.36417775131883712, 0.36389322218743292, 0.36361078112892431, 0.363330407035933, 0.36305207912970033, 0.3627757769523392, 0.36250148035932828, 0.36222916951223821, 0.36195882487168368, 0.36169042719049016, 0.36142395750706791, 0.36115939713898954, 0.3608967276767594, 0.36063593097776814, 0.36037698916043037, 0.36011988459849265, 0.35986459991551134, 0.35961111797948986, 0.35935942189767378, 0.35910949501149447, 0.35886132089165962, 0.35861488333338232, 0.35837016635174634, 0.35812715417720253, 0.35788583125119006, 0.35764618222188127, 0.35740819194004336, 0.35717184545501324, 0.35693712801078475, 0.35670402504220011, 0.35647252217124442, 0.35624260520344098, 0.35601426012434145, 0.35578747309610997, 0.35556223045419627, 0.35533851870409661, 0.35511632451819886, 0.35489563473270908, 0.35467643634465679, 0.35445871650897709, 0.35424246253566671, 0.35402766188701085, 0.35381430217488152, 0.35360237115810056, 0.35339185673986939, 0.35318274696526225, 0.35297503001877861, 0.35276869422195756, 0.35256372803104757, 0.35236012003473327, 0.35215785895191498, 0.35195693362954278, 0.35175733304049878, 0.35155904628153184, 0.35136206257123681, 0.35116637124808342, 0.35097196176848755, 0.3507788237049283, 0.35058694674410679, 0.35039632068514603, 0.35020693543783166, 0.35001878102089046, 0.34983184756030783, 0.3496461252876818, 0.34946160453861208, 0.34927827575112447, 0.34909612946412949, 0.34891515631591219, 0.34873534704265596, 0.34855669247699589, 0.34837918354660252, 0.34820281127279595, 0.348027566769187, 0.34785344124034756, 0.34768042598050736, 0.34750851237227637, 0.34733769188539443, 0.3471679560755056, 0.34699929658295398, 0.34683170513160755, 0.34666517352770282, 0.34649969365871069, 0.34633525749222688, 0.34617185707488179, 0.34600948453127239, 0.3458481320629132, 0.34568779194720745, 0.34552845653643827, 0.34537011825677766, 0.34521276960731384, 0.34505640315909747, 0.34490101155420405, 0.34474658750481402, 0.34459312379230878, 0.34444061326638503, 0.34428904884418143, 0.34413842350942347, 0.34398873031158289, 0.3438399623650506, 0.34369211284832535, 0.34354517500321569, 0.34339914213405587, 0.34325400760693531, 0.34310976484894096, 0.34296640734741202, 0.34282392864920813, 0.34268232235998841, 0.34254158214350483, 0.34240170172090439, 0.34226267487004403, 0.34212449542481727, 0.34198715727449158, 0.34185065436305506, 0.34171498068857553, 0.34158013030256845, 0.34144609730937547, 0.34131287586555265, 0.34118046017926823, 0.34104884450971035, 0.34091802316650222, 0.34078799050912945, 0.34065874094637183, 0.34053026893574889, 0.34040256898296906, 0.34027563564139029, 0.34014946351148823, 0.34002404724033053, 0.33989938152106164, 0.33977546109239332, 0.33965228073810377, 0.3395298352865429, 0.33940811961014594, 0.33928712862495403, 0.33916685729014051, 0.33904730060754557, 0.33892845362121748, 0.33881031141695755, 0.33869286912187629, 0.33857612190395142, 0.33846006497159498, 0.33834469357322472, 0.33823000299684269, 0.33811598856961872, 0.33800264565748012, 0.33788996966470708, 0.337777956033533, 0.33766660024375134, 0.33755589781232614, 0.33744584429300978, 0.33733643527596402, 0.33722766638738733, 0.33711953328914668, 0.33701203167841426, 0.33690515728730874, 0.33679890588254169, 0.3366932732650687, 0.3365882552697444, 0.33648384776498225, 0.33638004665241894, 0.33627684786658324, 0.33617424737456747, 0.33607224117570705, 0.33597082530125844, 0.33586999581408672, 0.33576974880835275, 0.3356700804092082, 0.33557098677249042, 0.33547246408442416, 0.33537450856132522, 0.33527711644930941, 0.3351802840240023, 0.33508400759025658, 0.33498828348186827, 0.33489310806130113, 0.33479847771940985, 0.33470438887517023, 0.33461083797541025, 0.33451782149454568, 0.334425335934319, 0.334333377823539, 0.33424194371782784, 0.33415103019936643, 0.33406063387664631, 0.33397075138422272, 0.33388137938247087, 0.33379251455734493, 0.33370415362014128, 0.33361629330726161, 0.33352893037998166, 0.33344206162422052, 0.33335568385031483, 0.33326979389279238, 0.3331843886101517, 0.33309946488464159, 0.33301501962204472, 0.33293104975146337, 0.3328475522251062, 0.33276452401807977, 0.33268196212818008, 0.33259986357568905, 0.33251822540317033, 0.33243704467526936, 0.3323563184785151, 0.33227604392112409, 0.33219621813280653, 0.33211683826457433, 0.33203790148855133, 0.33195940499778609, 0.331881346006066, 0.33180372174773376, 0.33172652947750692, 0.33164976647029581, 0.33157343002102846, 0.33149751744447342, 0.33142202607506671, 0.33134695326673913, 0.33127229639274647, 0.33119805284550136, 0.33112422003640601, 0.33105079539568849, 0.33097777637223891, 0.33090516043344786, 0.33083294506504796, 0.33076112777095434, 0.33068970607310988, 0.33061867751132912, 0.33054803964314644, 0.33047779004366334, 0.33040792630539995, 0.33033844603814533, 0.33026934686881138, 0.33020062644128723, 0.33013228241629594, 0.33006431247125129, 0.32999671430011768, 0.32992948561326962, 0.32986262413735534, 0.32979612761515831, 0.3297299938054632, 0.32966422048292132, 0.32959880543791847, 0.32953374647644379, 0.32946904141995909, 0.32940468810527124, 0.32934068438440395, 0.3292770281244724, 0.32921371720755799, 0.32915074953058543, 0.32908812300519918, 0.32902583555764364, 0.328963885128642, 0.32890226967327862, 0.3288409871608805, 0.32878003557490165, 0.32871941291280693, 0.32865911718595836, 0.32859914641950216, 0.32853949865225618, 0.32848017193659995, 0.32842116433836399, 0.32836247393672108, 0.32830409882407952, 0.32824603710597522, 0.32818828690096669, 0.32813084634053069, 0.32807371356895759, 0.32801688674324997, 0.32796036403301976, 0.32790414362038794, 0.32784822369988609, 0.32779260247835484, 0.32773727817484866, 0.32768224902053733, 0.32762751325861034, 0.32757306914418227, 0.32751891494419727, 0.32746504893733719, 0.32741146941392785, 0.32735817467584843, 0.32730516303643969, 0.32725243282041461, 0.32719998236376963, 0.32714781001369553, 0.32709591412849054, 0.32704429307747335, 0.32699294524089784, 0.32694186900986721, 0.32689106278625024, 0.32684052498259764, 0.32679025402205913, 0.3267402483383019, 0.32669050637542874, 0.32664102658789801, 0.32659180744044375, 0.32654284740799594, 0.32649414497560392, 0.32644569863835643, 0.32639750690130659, 0.32634956827939443, 0.32630188129737259, 0.32625444448972968, 0.32620725640061771, 0.32616031558377778, 0.32611362060246707, 0.32606717002938707, 0.32602096244661161, 0.3259749964455162, 0.32592927062670701, 0.32588378359995274, 0.32583853398411305, 0.32579352040707277, 0.32574874150567201, 0.32570419592563959, 0.32565988232152676, 0.32561579935664037, 0.32557194570297787, 0.32552832004116189, 0.32548492106037619, 0.3254417474583019, 0.3253987979410542, 0.32535607122311877, 0.32531356602729106, 0.32527128108461389, 0.32522921513431569, 0.3251873669237515, 0.325145735208341, 0.32510431875151086, 0.32506311632463458, 0.32502212670697356, 0.32498134868562067, 0.32494078105544144, 0.32490042261901686, 0.32486027218658781, 0.32482032857599868, 0.32478059061264092, 0.32474105712939944, 0.3247017269665966, 0.32466259897193861, 0.3246236720004616, 0.32458494491447898, 0.3245464165835279, 0.32450808588431673, 0.32446995170067389, 0.32443201292349572, 0.32439426845069563, 0.32435671718715353, 0.32431935804466522, 0.32428218994189328, 0.32424521180431692, 0.32420842256418358, 0.32417182116045989, 0.32413540653878398, 0.32409917765141733, 0.32406313345719767, 0.32402727292149125, 0.32399159501614794, 0.3239560987194528, 0.32392078301608235, 0.3238856468970574, 0.32385068935969907, 0.32381590940758365, 0.32378130605049832, 0.32374687830439719, 0.32371262519135741, 0.32367854573953658, 0.32364463898312934, 0.32361090396232423, 0.32357733972326291, 0.32354394531799696, 0.32351071980444701, 0.32347766224636143, 0.32344477171327535, 0.32341204728046985, 0.3233794880289329, 0.32334709304531756, 0.32331486142190441, 0.32328279225656054, 0.32325088465270224, 0.32321913771925487, 0.32318755057061571, 0.32315612232661517, 0.32312485211247971, 0.32309373905879357, 0.32306278230146285, 0.32303198098167774, 0.32300133424587585, 0.32297084124570707, 0.32294050113799666, 0.32291031308470997, 0.32288027625291704, 0.32285038981475711, 0.32282065294740431, 0.32279106483303277, 0.32276162465878316, 0.3227323316167271, 0.32270318490383426, 0.32267418372194012, 0.32264532727771006, 0.32261661478260878, 0.32258804545286629, 0.32255961850944609, 0.32253133317801275, 0.32250318868889993, 0.32247518427707866, 0.3224473191821256, 0.32241959264819287, 0.32239200392397593, 0.32236455226268323, 0.32233723692200594, 0.32231005716408756, 0.32228301225549372, 0.32225610146718248, 0.3222293240744753, 0.32220267935702634, 0.32217616659879506, 0.32214978508801589, 0.32212353411717076, 0.32209741298295935, 0.32207142098627184, 0.32204555743216079, 0.32201982162981291, 0.3219942128925215, 0.32196873053766001, 0.32194337388665328, 0.32191814226495152, 0.32189303500200384, 0.32186805143123104, 0.32184319088999919, 0.32181845271959397, 0.32179383626519464, 0.32176934087584796, 0.3217449659044424, 0.3217207107076831, 0.32169657464606771, 0.32167255708385867, 0.32164865738906145, 0.32162487493339764, 0.32160120909228163, 0.3215776592447962, 0.32155422477366835, 0.32153090506524473, 0.32150769950946911, 0.3214846074998583, 0.32146162843347797, 0.32143876171092112, 0.32141600673628296, 0.32139336291714027, 0.32137082966452712, 0.32134840639291251, 0.32132609252017863, 0.3213038874675983, 0.3212817906598131, 0.32125980152481121, 0.32123791949390579, 0.32121614400171333, 0.32119447448613281, 0.32117291038832396, 0.32115145115268617, 0.32113009622683725, 0.32110884506159371, 0.32108769711094909, 0.32106665183205413, 0.32104570868519589, 0.32102486713377809, 0.32100412664430072, 0.32098348668634002, 0.32096294673252918, 0.32094250625853921, 0.32092216474305746, 0.32090192166777087, 0.32088177651734473, 0.320861728779405, 0.3208417779445184, 0.3208219235061745, 0.32080216496076674, 0.32078250180757351, 0.32076293354874036, 0.32074345968926177, 0.32072407973696238, 0.32070479320247958, 0.32068559959924597, 0.32066649844347062, 0.32064748925412234, 0.32062857155291213, 0.32060974486427501, 0.32059100871535423, 0.32057236263598288, 0.32055380615866713, 0.32053533881857021, 0.32051696015349485, 0.32049866970386698, 0.32048046701271876, 0.32046235162567316, 0.32044432309092652, 0.32042638095923354, 0.32040852478389031, 0.32039075412071882, 0.32037306852805092, 0.32035546756671329, 0.32033795080001054, 0.32032051779371112, 0.32030316811603077, 0.32028590133761853, 0.32026871703154003, 0.32025161477326358, 0.32023459414064492, 0.32021765471391239, 0.32020079607565172, 0.3201840178107922, 0.3201673195065915, 0.32015070075262131, 0.32013416114075344, 0.32011770026514497, 0.32010131772222472, 0.32008501311067844, 0.32006878603143574, 0.32005263608765555, 0.32003656288471244, 0.3200205660301837, 0.32000464513383459, 0.31998879980760575, 0.31997302966559954, 0.31995733432406687, 0.31994171340139388, 0.31992616651808836, 0.31991069329676758, 0.31989529336214517, 0.31987996634101779, 0.31986471186225263, 0.31984952955677504, 0.31983441905755566, 0.31981937999959781, 0.31980441201992549, 0.31978951475757028, 0.31977468785356056, 0.3197599309509071, 0.31974524369459356, 0.31973062573156258, 0.31971607671070473, 0.31970159628284639, 0.31968718410073793, 0.31967283981904238, 0.31965856309432344, 0.31964435358503479, 0.31963021095150712, 0.31961613485593837, 0.31960212496238122, 0.31958818093673297, 0.31957430244672363, 0.31956048916190449, 0.31954674075363892, 0.3195330568950891, 0.3195194372612073, 0.31950588152872333, 0.31949238937613528, 0.319478960483698, 0.319465594533413, 0.31945229120901752, 0.31943905019597502, 0.31942587118146354, 0.31941275385436635, 0.31939969790526201, 0.31938670302641303, 0.31937376891175689, 0.31936089525689543, 0.31934808175908541, 0.31933532811722826, 0.3193226340318604, 0.31930999920514347, 0.31929742334085426, 0.31928490614437643, 0.319272447322689, 0.31926004658435897, 0.31924770363952959, 0.31923541819991336, 0.31922318997878107, 0.31921101869095281, 0.31919890405278978, 0.31918684578218404, 0.31917484359855008, 0.31916289722281499, 0.31915100637741117, 0.3191391707862658, 0.31912739017479252, 0.31911566426988336, 0.31910399279989859, 0.31909237549466002, 0.31908081208544037, 0.31906930230495645, 0.31905784588735908, 0.31904644256822606, 0.31903509208455322, 0.31902379417474591, 0.31901254857861089, 0.31900135503734839, 0.31899021329354332, 0.31897912309115811, 0.31896808417552358, 0.31895709629333158, 0.31894615919262725, 0.31893527262280036, 0.3189244363345779, 0.31891365008001676, 0.31890291361249506, 0.3188922266867048, 0.31888158905864444, 0.31887100048561162, 0.31886046072619434, 0.31884996954026484, 0.31883952668897114, 0.3188291319347305, 0.31881878504122119, 0.31880848577337612, 0.31879823389737416, 0.31878802918063537, 0.31877787139181057, 0.31876776030077708, 0.31875769567862977, 0.31874767729767539, 0.31873770493142439]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cl.losses_)\n",
    "plt.show()\n",
    "print(cl.losses_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing (8 points)\n",
    "\n",
    "##  Kaggle \n",
    "\n",
    "Kaggle is a website dedicated to machine learning. There is a large number of datasets.\n",
    "Competitions are organized by companies and organisations. These provide a dataset and a goal. The \"kagglers\" who participate in these competitions submit their results online. There are often prices or jobs for those who get the best results.\n",
    "\n",
    "This is a good way to develop machine learning skills on real datasets.\n",
    "\n",
    "You can create an account if you want to compare your results to those already online for the dataset we are going to study.\n",
    "\n",
    "You can create an account here: https://www.kaggle.com/\n",
    "\n",
    "## Austin Animal Center Shelter Animal Outcomes dataset\n",
    "The dataset that we will use is the \"Animal Outcomes dataset\" available at the following address: https://www.kaggle.com/c/shelter-animal-outcomes.\n",
    "\n",
    "This is a problem of **multi-class classification** where animals are collected in a shelter after being abandoned, the purpose is to predict how they will \"leave\" the place:\n",
    "* Adoption\n",
    "* Back to the owner\n",
    "* Death\n",
    "* Euthanasia\n",
    "* Transfer to another center\n",
    "\n",
    "For more information on data, go to kaggle.\n",
    "\n",
    "## Structure of a machine learning project\n",
    "\n",
    "The goal of the this part of the lab is to make you study a simplified version of a complete machine learning project:\n",
    "\n",
    "1. Data cleaning, missing value processing\n",
    "2. Formatting Data for Use in Machine Learning Algorithms\n",
    "3. Feature engineering transformation or feature combinations between them\n",
    "4. Comparison of the performances of the different choices made during the data processing\n",
    "5. Comparison of the performances of different models (including the one implemented in the first part)\n",
    "6. Optimization of hyper-parameters\n",
    "\n",
    "\n",
    "## Scikit-learn\n",
    "http://scikit-learn.org/stable/\n",
    "\n",
    "It is a machine learning and data mining library, it offers tools for data analysis and processing, classical machine learning algorithms such as neural networks, logistic regression, SVM or other, finally tools to compare models between them such as cross validation.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "A library to store and manipulate data easily\n",
    "\n",
    "The two basic elements of pandas are the dataframe and the series.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.html\n",
    "\n",
    "## Data processing tutorial\n",
    "\n",
    "** Before continuing the lab **, familiarize yourself with the **pre-processing data**, **pandas** and **scikit-learn**, a tutorial is available in the file: *data_processing_tutorial.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### Load train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"data/\"\n",
    "X_train = pd.read_csv(PATH + \"train.csv\")\n",
    "X_test = pd.read_csv(PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useless features removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"OutcomeSubtype\",\"AnimalID\"])\n",
    "X_test = X_test.drop(columns = [\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.drop(columns = [\"OutcomeType\"]),X_train[\"OutcomeType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hambone</td>\n",
       "      <td>2014-02-12 18:22:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily</td>\n",
       "      <td>2013-10-13 12:44:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearce</td>\n",
       "      <td>2015-01-31 12:28:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-11 19:09:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-15 12:52:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0  Hambone  2014-02-12 18:22:00        Dog  Neutered Male         1 year   \n",
       "1    Emily  2013-10-13 12:44:00        Cat  Spayed Female         1 year   \n",
       "2   Pearce  2015-01-31 12:28:00        Dog  Neutered Male        2 years   \n",
       "3      NaN  2014-07-11 19:09:00        Cat    Intact Male        3 weeks   \n",
       "4      NaN  2013-11-15 12:52:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                         Breed        Color  \n",
       "0        Shetland Sheepdog Mix  Brown/White  \n",
       "1       Domestic Shorthair Mix  Cream Tabby  \n",
       "2                 Pit Bull Mix   Blue/White  \n",
       "3       Domestic Shorthair Mix   Blue Cream  \n",
       "4  Lhasa Apso/Miniature Poodle          Tan  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer</td>\n",
       "      <td>2015-10-12 12:15:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>10 months</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>Red/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>2014-07-26 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>2 years</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>Black/Tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gus</td>\n",
       "      <td>2016-01-13 12:20:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Brown Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pongo</td>\n",
       "      <td>2013-12-28 18:12:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>4 months</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>Tricolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skooter</td>\n",
       "      <td>2015-09-24 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0    Summer  2015-10-12 12:15:00        Dog  Intact Female      10 months   \n",
       "1  Cheyenne  2014-07-26 17:59:00        Dog  Spayed Female        2 years   \n",
       "2       Gus  2016-01-13 12:20:00        Cat  Neutered Male         1 year   \n",
       "3     Pongo  2013-12-28 18:12:00        Dog    Intact Male       4 months   \n",
       "4   Skooter  2015-09-24 17:59:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                            Breed        Color  \n",
       "0          Labrador Retriever Mix    Red/White  \n",
       "1  German Shepherd/Siberian Husky    Black/Tan  \n",
       "2          Domestic Shorthair Mix  Brown Tabby  \n",
       "3               Collie Smooth Mix     Tricolor  \n",
       "4            Miniature Poodle Mix        White  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Return_to_owner\n",
       "1         Euthanasia\n",
       "2           Adoption\n",
       "3           Transfer\n",
       "4           Transfer\n",
       "Name: OutcomeType, dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "To save you time, some of the columns (Name, DateTime, color) have already been processed.\n",
    "\n",
    "\n",
    "Using the tutorial provided, you must write a complete transformation pipeline for each of the remaining columns in the dataset (AgeuponOutcome, AnimalType, SexuponOutcome, Breed).\n",
    "\n",
    "You are **free** of your choices, but you must **justify** column by column.\n",
    "For example, you can choose to combine columns with each other, separate a column or eliminate a column completely if you correctly justify it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The already preprocessed part of the dataset is loaded in **X_train1** and **X_test1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.read_csv(\"data/train_preprocessed.csv\")\n",
    "X_test1 = pd.read_csv(\"data/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>HasName</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.421532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.471381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.868974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Color  HasName  Month  Day  Hour\n",
       "0  0.973624      1.0    2.0  1.0   3.0\n",
       "1 -1.421532      1.0   10.0  1.0   2.0\n",
       "2  0.973624      1.0    1.0  3.0   2.0\n",
       "3 -1.471381      0.0    7.0  1.0   3.0\n",
       "4 -0.868974      0.0   11.0  1.0   2.0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset part you have to process is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "X_test = X_test.drop(columns = [\"Color\",\"Name\",\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome AgeuponOutcome                        Breed\n",
       "0        Dog  Neutered Male         1 year        Shetland Sheepdog Mix\n",
       "1        Cat  Spayed Female         1 year       Domestic Shorthair Mix\n",
       "2        Dog  Neutered Male        2 years                 Pit Bull Mix\n",
       "3        Cat    Intact Male        3 weeks       Domestic Shorthair Mix\n",
       "4        Dog  Neutered Male        2 years  Lhasa Apso/Miniature Poodle"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 11: AgeuponOutcome (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: AnimalType (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: SexuponOutcome (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14: Breed (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "**Question 15: Fill the pipeline below (4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TransformationWrapper\n",
    "from preprocessing import LabelEncoderP\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline_color = Pipeline([\n",
    "#     (\"name\", Transformer()),\n",
    "# ])\n",
    "\n",
    "\n",
    "# full_pipeline = ColumnTransformer([\n",
    "#         (\"color\", pipeline_color, [\"Color\"]),\n",
    "        \n",
    "\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = []\n",
    "# X_train_prepared = pd.DataFrame(full_pipeline.fit_transform(X_train),columns = columns)\n",
    "# X_test_prepared = pd.DataFrame(full_pipeline.fit_transform(X_test),columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate both part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train1,X_train_prepared], axis = 1)\n",
    "# X_test = pd.concat([X_test1,X_test_prepared], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the target class as integers to use it\n",
    "with scikit-learn algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_label = LabelEncoder()\n",
    "y_train_label = target_label.fit_transform(y_train)\n",
    "print(target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set\n",
    "\n",
    "To compare different models with each other, we can not use the test set, otherwise one would be tempted to keep the model corresponding best to the test set which could lead to overfitting.\n",
    "\n",
    "It is common to create a new set of the size of the test set, the  **validation** set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Cross-validation is a useful method for comparing the performance of different machine learning models **without creating a validation set**.\n",
    "\n",
    "There are different types of cross-validation, the most classic procedure is:\n",
    "* Randomly divide the training set into two parts (90% / 10% for example).\n",
    "* Train the model on biggest part, and test it on the other part.\n",
    "* Repeat n times\n",
    "* Calculate the mean and standard deviation of the results\n",
    "\n",
    "The benefits are:\n",
    "* Consider the entire training set for the evaluation (without ignoring the data we would have use in the validation set)\n",
    "* Obtaining the standard deviation of the results allows a better evaluation of the model's accuracy.\n",
    "\n",
    "The main disadvantage is the computation time, since one carries out the learning of the model several times, this method can be impossible for datasets containing a large number of example (> 10e5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: StratifiedKFold (1 point)\n",
    "\n",
    "By observing the class distribution of the target attribute (using the pandas visualization functions), justify the use of the sklearn **StratifiedKFold** object for division of the training set when doing cross-validation instead of a pure **random** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The reason why we use StratifiedKFold is that if we do so we can keep the same ratios of each class in the\n",
    "test set as the ratios of the train set. This is very important because essentially we want to verify the model on a test set\n",
    "which is similar to the train set. Respecting the proportional of each class in the test set is a guarantee for the similarity. \n",
    "If we use a random method, it would not keep the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: (1 point)\n",
    "\n",
    "\n",
    "**Choose at least two models allowing the multiclass classification on sklearn in addition to the model implemented in the first part of the TP**.\n",
    "\n",
    "**Complete the compare function that performs the crossvalidation for different models and different metrics, and returns the list of averages and standard deviations for each of the metrics, for each of the models.**\n",
    "\n",
    "**Based on the different metrics, conclude on the best performing model.**\n",
    "\n",
    "Evaluate the models for the different metrics proposed:\n",
    "* **log loss**: this is the kaggle evaluation metric for this dataset\n",
    "* **precision**: corresponds to the quality of the prediction, the number of classes correctly predicted by the total prediction number\n",
    "* **recall**: the number of elements belonging to a class, identified as such, divided by the total number of elements of that class.\n",
    "* **f-score**: an average of accuracy and recall\n",
    "\n",
    "**Note: Precision and recall are two complementary measures for evaluating a multi-class classification model.**\n",
    "\n",
    "In the case of a binary classification with an important target class imbalance, (90% / 10%), evaluating the classification result with accuracy (number of correct predictions divided by the total number of predictions), a very good score (90% accuracy) can be obtained by choosing to systematically predict the majority class.\n",
    "\n",
    "In such a case, the precision would be high in the same way, but the recall would be very low, indicating the mediocrity of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(models,X_train,y_train,nb_runs):\n",
    "    losses = []\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "nb_run = 3\n",
    "\n",
    "models = [\n",
    "    SoftmaxClassifier(),\n",
    "]\n",
    "\n",
    "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
    "\n",
    "compare(models,X_train,y_train_label,nb_run,scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: Confusion matrix (0.5 point)\n",
    "\n",
    "The confusion matrix A is such that $A_{i,j}$ represents the number of examples of class i classified as belonging to class j.\n",
    "\n",
    "Train the selected model on the entire training set.\n",
    "Using the confusion matrix and class distribution, analyze in more detail the performance of the chosen model and justify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected model\n",
    "\n",
    "selected_model = \n",
    "y_pred = selected_model.fit_predict(X_train,y_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_train_label, y_pred), columns = target_label.classes_, index = target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Target class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(target_label.classes_)\n",
    "pd.Series(y_train_label).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3: Hyper-parameters optimization (1 point)\n",
    "\n",
    "Hyper-parameters are the parameters set before the learning phase. To optimize the performance of the model, we can select the best hyper-parameters.\n",
    "\n",
    "Using sklearn, optimize the hyper-parameters of the model you have selected and show that the performance has been improved.\n",
    "For example, you can use: **GridSearchCV**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make the prediction on the test set and give your results when submitting the lab.\n",
    "\n",
    "**Optional**: You can submit your results on kaggle and note your performance in terms of log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = \n",
    "# pred_test = pd.Series(best_model.transform(X_test))\n",
    "# pred_test.to_csv(\"test_prediction.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
